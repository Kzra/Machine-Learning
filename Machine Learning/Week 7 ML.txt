Machine Learning Week 7: Support Vector Machines 
Generally the supervised learning algorithm you use is less important than the data and features you feed.
One additional algorithm is support vector machines (as opposed to logistic regression/neural networks).

Logistic regression: h(x) = 1/1+e^-theta'X 
if y ==1 we want h(x) = 1, theta'x >> 0
Support vector machine uses a modified cost function in which cost is zero when y ==1 & z(theta'x) is > 1 (as opposed to just a small value). This is computationally advantageous for optimisation. and vice versa for z < -1.
Only looks at z -1 to 1! 

SVM uses two cost terms to replace the h(x) in the log reg cost function. 
J = 1/m*sum(ycost1(z) + (1-y)cost0(z) + lambda/2m(sum(theta)). 
this is modified to fit the SVM convention:
-remove 1/m (it's a constant, doesn't affect the optimal value of theta)
-use C parameter instead of lambda in context CA + B (lambda: A + lambdaB). Thus high C means high variance. (C can be thought of as 1/lambda).
optimisation: min C*sum(y(i)*cost1(z) + (1-y(i)cost0(z)) + 1/2*sum(theta^2). 

SVM also uses a different hypothesis 
h(x) = 1 or 0 (not a probability like log reg)

SVM are known as large margin classifiers: 
if y ==1 theta'X has to be bigger than 1 for cost == 0. think of like extra safety margin in the machine. 
the decision boundary has a 'larger margin' which is the minimum distance from any of the training examples. this gives a robustness. svm will use as large a margin as it can

if you set a small value of C you account for anomolous values. large margin therefore only applies when C is very large. 

Mathematics behind large margin classification: 
u = [u1;u2]
||u|| = norm of u = euclidean length of u = sqrt(u1^2 + u2^2) = R
v = [v1;v2]
||v||
vector inner product 
p . ||u|| 
p is signed, depends on the angle between u and v 

